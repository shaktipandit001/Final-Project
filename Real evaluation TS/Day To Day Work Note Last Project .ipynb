{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d54e39",
   "metadata": {},
   "source": [
    "# Work Progress\n",
    "\n",
    "## - Last Project of Bootcamp \n",
    "\n",
    "* Go through the plan and project overview.\n",
    "* <u>Talk with Andre and make a mindset about the steps I need to take and what data I am using.</u>\n",
    "* Set a target for what I would like to achieve with my dataset.\n",
    "* Did not do a lot of work, only thought about and made a mindset about what I need to do.\n",
    "\n",
    "#### Most Difficult Part is Collecting Data\n",
    "\n",
    "## - Last Project of Bootcamp \n",
    "\n",
    "* I am using data from my company, so I need to gather all necessary information.\n",
    "* For my COGS (Cost of Goods Sold) app and forecasting, I require a significant amount of data. I have selected some key data points from our company's website.\n",
    "* Our company utilizes BOOTSTraper (Backoffice) software provided by Samuelson Kassen, a German company specializing in cash handling and offering various functions tailored for food companies.\n",
    "* As part of my plan, I have compiled a list of essential data and included some notes on it.\n",
    "\n",
    "#### Divided into Two Parts:\n",
    "\n",
    "* **COGs Part**:\n",
    "    1. Calculation of all recipes, which I need to do myselfâ€”very time-consuming process.\n",
    "    2. Total product sales: necessary for accurate data calculation.\n",
    "    3. Total goods purchased in 2023.\n",
    "    4. Total goods sales of 2023 (requires recipe calculations).\n",
    "    5. Inventory details for all products made by our company.\n",
    "    6. Total goods list including actual price, article number, article name, supplier, packaging/mass, article category, purchase date, and quantity bought.\n",
    "    7. Waste management data.\n",
    "\n",
    "* **Sales Part for Forecasting**:\n",
    "    1. Daily sales per recipe statistics, detailing multiple important data points.\n",
    "    2. Daily sales throughout the year.\n",
    "    3. Total hours worked by each worker.\n",
    "    4. Total discounts and coupons issued.\n",
    "    5. Daily detailed product sales.\n",
    "    6. Overall daily sales total.\n",
    "\n",
    "# ETL (Extract Transform Load) Process\n",
    "\n",
    "#### Data Collection Part\n",
    "\n",
    "## <u>Data Collected From Braun Food Services GmbH.</u>\n",
    "#### <u>(Samuelson Kassensysteme) - Bootstraper</u>\n",
    "#### <u>Sales Data from 2023 (Real Data)</u>\n",
    "\n",
    "* **COGs Part**:\n",
    "    1. Calculation of all recipes, which I started but found it to be a very time-consuming process (estimated 2-3 weeks). Instead, I decided to create a demo with sample data to showcase its impact on other affected data.\n",
    "    2. Total product sales: extracted from the company's STastic or Umsastzt (Sales File).\n",
    "    3. Total goods purchased in 2023: received monthly from our Goods Sales Department, ensuring it is current and accurate.\n",
    "    4. Total goods sales of 2023: I created a demo version with approximate figures since full data requires recipes.\n",
    "    5. Inventory details for all products made by our company: obtained monthly from our Store Manager.\n",
    "    6. Total goods list including actual price, article number, article name, supplier, packaging/mass, article category, purchase date, and quantity bought: updated monthly from our Goods Sales Department.\n",
    "    7. Waste management data: provided demo data for waste and lost or stolen items combined into one category.\n",
    "\n",
    "* **Sales Part for Forecasting**:\n",
    "    1. Daily sales per recipe statistics: sourced from the company's Backoffice.\n",
    "    2. Daily sales throughout the year: extracted from the company's Backoffice. Split into 6 parts due to download restrictions.\n",
    "    3. Total hours worked by each worker: retrieved from the company's Backoffice.\n",
    "    4. Total discounts and coupons issued: data collected from the company's Backoffice.\n",
    "    5. Daily detailed product sales: obtained from the company's Backoffice.\n",
    "    6. Overall daily sales total: obtained from the company's Backoffice.\n",
    "\n",
    "### I collected 18 Excel files for my work\n",
    "\n",
    "### Reviewing The Data Display\n",
    "\n",
    "* I am going through every piece of data orally, and I've discovered that it's very extensive. I'll need to filter through all of it.\n",
    "* Upon review, I've decided to omit the Daily sales throughout the year, which was extracted from the company's Backoffice and split into 6 parts due to download restrictions. Instead, I'll use the Overall Daily sales, which contains the same data with slight differences that won't affect anything, so I've chosen not to include it.\n",
    "* Therefore, I am ignoring the Price List, as it is unnecessary for my needs.\n",
    "* I am also ignoring the List of Goods for Purchase because it is already included in the Total Buying of Goods.\n",
    "\n",
    "### This means I  have only 8 Excel files for my work.\n",
    "\n",
    "#### <u> Andre's Tips to Merge All Data and Preprocess It </u>\n",
    "\n",
    "* I am merging and uploading Excel data, and the dataset looks very messy. So, I have delved deeper into the dataset and removed all columns from each Excel file that do not have any value and are not necessary for me. This is because merging it caused the data to become messy. I spent almost the whole night preprocessing all the data once more in Excel. Now, I'm excited for the next day.\n",
    "\n",
    "\n",
    "## Day 3 - Last Project of Bootcamp 22.06.2024\n",
    "* When I started To Merge it Takes a lot of time and brak Down in Middel. \n",
    "* Error That I Got \n",
    "##### emoryError: Unable to allocate 5.27 GiB for an array with shape (13, 54419652) and data type float64\n",
    "* I got This Error while Merging 4 File.\n",
    "* next I am merging only 2 File also get Error.\n",
    "* Error That I Got  Next\n",
    "##### MemoryError: Unable to allocate 1.23 GiB for an array with shape (9, 18364725) and data type float64\n",
    "\n",
    "#### <u> Andre's introduce me New Library Called Polar </u>\n",
    "\n",
    "* I need To Go Through Documntation of Polar So I can shout the Merging Trouble.\n",
    "\n",
    "\n",
    "## Polars Library in Python\n",
    "\n",
    "### Overview\n",
    "Polars is a high-performance DataFrame library for Python, optimized for large datasets and efficient data manipulation.\n",
    "\n",
    "### Key Features\n",
    "- **High Performance**: Utilizes multi-threading and SIMD for fast data processing with low memory usage.\n",
    "- **DataFrame Operations**:\n",
    "  - **Lazy Evaluation**: Defers execution to optimize performance.\n",
    "  - **Eager Evaluation**: Executes operations immediately, similar to pandas.\n",
    "  - **Streaming**: Processes large datasets in chunks to handle memory constraints.\n",
    "- **User-Friendly API**: Intuitive and similar to pandas, making it easy for users to adopt.\n",
    "- **Interoperability**: Compatible with pandas and integrates well with other data processing libraries.\n",
    "\n",
    "### Benefits\n",
    "- **Speed**: Designed to be significantly faster than pandas.\n",
    "- **Efficiency**: Handles large datasets with efficient memory usage.\n",
    "- **Flexibility**: Supports complex data manipulation and queries.\n",
    "\n",
    "Polars offers a powerful alternative to pandas, focusing on performance and scalability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0cf61",
   "metadata": {},
   "source": [
    "# Preprocessing with Polar \n",
    "\n",
    "#### Doing all preproceing. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### * After uploading all needed data, join the tables.\n",
    "###### * After joining, the result has 49 million rows and 38 columns.\n",
    "###### * During preprocessing, I encountered many problems because the data is too big.\n",
    "###### * It always crashes.\n",
    "###### * Almost all the time, it breaks the whole process.\n",
    "###### * At some points, I cannot save it, and I have to redo the entire process repeatedly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### After this all problem, I decided To do it seperatly because My Target is to predict sales , for that I need Date and selling Prises. Which I will get it from last Part of Data.\n",
    "\n",
    "\n",
    "\n",
    "# <u> Now I am doing things totally differently than planned.</u>\n",
    "\n",
    "### Now I have three parts, and three folders for different evaluations and understandings, as well as visualizations.\n",
    "\n",
    "## 1: First Part - COGS (Cost of Goods Sold)\n",
    "#### * There are three Excel files: Total Sales of Product, Inventory, and Waste.\n",
    "\n",
    "##### Objectives:\n",
    "\n",
    "###### 1. \n",
    " - Total number of products bought (Anzahl).\n",
    " - Total cost in Euros (EUR).\n",
    "\n",
    "###### 2. \n",
    " - The highest number of goods bought in a single transaction over the whole year.\n",
    "\n",
    "###### 3. \n",
    " - Top 5 groups of products that are bought the most.\n",
    " - Bottom 5 groups of products that are bought the least.\n",
    " - Top 5 products that are bought the most.\n",
    " - Bottom 5 products that are bought the least.\n",
    "\n",
    "###### 4. \n",
    " - Top 5 costly products that are bought (EUR).\n",
    " - Bottom 5 least costly products that are bought (EUR).\n",
    "\n",
    "\n",
    "\n",
    "## 2: Second Part is Daily Products that are sold in the year 2023\n",
    "#### * There are 6 Exel files, when I was extracting Data from company Software, it did not give me permission to extract a whole year's Data so I had to make it all one Dta within 2 months. After that, I have to join all in one. \n",
    "\n",
    "##### Objectives:\n",
    "\n",
    "###### 1.\n",
    "- Total Sales Calculation\n",
    "\n",
    "###### 2.\n",
    "- Identify Top 10 Selling Products\n",
    "- Identify the Top Bottom 10 Selling Products\n",
    "\n",
    "###### 3.\n",
    "- Daily Total Minimum and maximum sales Product- With adjacent Date\n",
    "- Products that are sold on the day and which are less sold or have 0 solved.\n",
    "\n",
    "###### 4. Core Article Analysis- Doing all the same steps as before.\n",
    "- Filter articles containing 'Kater' or 'Bowl' in their names.\n",
    "- Calculate total core products sold and visualize the top 5 and bottom 5 core articles.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3: Last Part ( Important part of the Analysis)\n",
    "#### This part contains massive Data, all the problems that I getting from this data because it also has important Data inside, which I need for the whole process, I can not ignore One Data to get a perfect result. \n",
    "\n",
    "#### This is the challenging part. After doing Join and further analysis it gives me the same error as in the saturating phase of doing all the things so I have decided on doing all the things separately, Taking only the necessary Data and completing Them for <u> Ml Part-Time Serie </u>\n",
    "\n",
    "##### There are Six Exel. \n",
    "###### 1. Workers Hours Part.\n",
    "###### 2. Vouchers and Discounts.\n",
    "###### 3. Sales.\n",
    "###### 4. Statistic, which will be the Machine Learning Part. (Time Series)\n",
    "\n",
    "### Before I started to do Further, I needed to Install so many Libraries which was not done before.\n",
    "\n",
    "\n",
    "##### Objectives:\n",
    "\n",
    "###### 1. Daily Sales Overview. (From Umsatzt Part)\n",
    "- Daybook Overview in Plot\n",
    "- Top 10 Highest Sales  Days\n",
    "- Top 10 Bottom Sales Days\n",
    "\n",
    "\n",
    "###### 2. Vouchers and Discounts. ( From Sanifair-Rabat Part)\n",
    "- Total Value  Count by Voucher and Discount With  Date\n",
    "- Top 10 Highest and Lowest Gutschein Total value with Date\n",
    "\n",
    "\n",
    "###### 3. Working Hour, Worker, and Total Hours of the Year. ( From Arbeit-Stunden- 2023)\n",
    "- Total Time of Working hour by Worker Name (in hours)\n",
    "- To and Bottom Daily Summary: Total Work Hours, Total Workers, Avg Work Hours per Worker\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Last Part Machine Learning Parts\n",
    "##### I am having a problem Join them all Together. So for this Part, I am taking Only two Exel, \n",
    "#### Feture Are - TotalSale and Datum\n",
    "\n",
    "##### Objectives:\n",
    "\n",
    "##### 1.Joining Statistics and Daybook, as Sales Data\n",
    "\n",
    "##### 2. Unloading Releted Library.\n",
    "\n",
    "##### 4. Specifying only related columns.(Date, Menge(number of product), Vk-precise(selling price of each goods))\n",
    "\n",
    "##### 5. From These specifying only - Datum, TotalPrice, Total Menge-(Each Day's total sales price for the total number of Goods)\n",
    "\n",
    "##### 6. Convert Polars DataFrame to pandas for further processing\n",
    "\n",
    "##### 7.  Log transformation and differencing\n",
    "\n",
    "##### 8.  Line Plot Total Cost\n",
    "\n",
    "##### 9.  Train/test split\n",
    "        - X_train = train(['Datum','TotalPrice'])\n",
    "        - y_train = train['TotalPrice']\n",
    "\n",
    "##### 10. Subplot Train/test split\n",
    "\n",
    "##### 11.\n",
    "- Create an instance of XGBRegressor with specified hyperparameters\n",
    "\n",
    "- reg = xgb.XGBRegressor\n",
    "   -    base_score=0.5,             # The initial prediction score for all instances, global bias\n",
    "   -    booster='gbtree',           # Specify which booster to use: 'gbtree', 'gblinear', or 'dart'\n",
    "   -    n_estimators=2000,          # The number of boosting rounds (trees) to fit\n",
    "   -    early_stopping_rounds=50,   # Stop training if validation score doesn't improve for 50 rounds\n",
    "   -    objective='reg: squared error,# Objective function for regression tasks (squared error)\n",
    "   -    max_depth=3,                # Maximum depth of a tree, controls model complexity\n",
    "   -    learning_rate=0.01,         # Step size shrinkage used to prevent overfitting\n",
    "   -    colsample_bytree=1,         # Subsample ratio of columns when constructing each tree\n",
    "   -    subsample=0.7               # Subsample ratio of the training instances\n",
    "\n",
    "\n",
    "- Train the model on the training data (X_train, y_train)\n",
    "- eval_set is used to monitor the performance of the model on the training and test data\n",
    "\n",
    "\n",
    "\n",
    "##### 12. <u> from sklearn.ensemble - RandomForestRegressor , import xgboost - xgbfrom, Prophet , sklearn.metrics - mean_absolute_error, mean_squared_error, from sklearn.model_selection - train_test_split, from sklearn.metrics - r2_score </u>\n",
    "* To find Out \n",
    "- print('Mean Absolute Error:', mae)\n",
    "- print('Mean Squared Error:', mse)\n",
    "- print('Root Mean Squared Error:', rmse)\n",
    "- print('R2:', r2)\n",
    "\n",
    "\n",
    "##### 13. Plot expected vs. predicted\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32d1f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "602297ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4e5f67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "141d3b9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb494db7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
